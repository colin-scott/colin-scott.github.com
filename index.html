
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Rest for the Wicked</title>
  <meta name="author" content="Colin Scott">

  
  <meta name="description" content="Thoughts on distributed systems">
  
  <meta name="keywords" content="SDN, software-defined networking, distributed systems, troubleshooting, Berkeley, Colin Scott, Scott Shenker">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://colin-scott.github.io">
  <link href="http://cryptm.org/~hpb/images/C.favicon.ico" rel="icon">
  <link href="/assets/css/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/assets/css/bootstrap-responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Rest for the Wicked" type="application/atom+xml">
  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-29849261-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">Rest for the Wicked</a>

      <div class="nav-collapse">
        <div class="navbar navbar-fixed-top">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>
      <a class="brand" href="http://www.eecs.berkeley.edu/~rcs/index.html">Colin Scott</a>
      <div class="nav-collapse collapse">
        <ul class="nav">
          <li><a href="http://www.eecs.berkeley.edu/~rcs/index.html">Home</a></li>
          <li><a href="http://www.eecs.berkeley.edu/~rcs/about.html">About</a></li>
          <li><a href="http://www.eecs.berkeley.edu/~rcs/contact.html">Contact</a></li>
          <li><a href="http://www.eecs.berkeley.edu/~rcs/publications.html">Publications</a></li>
          <li class="active"><a href="http://colin-scott.github.com/">Blog</a></li>
          <li><a href="http://www.eecs.berkeley.edu/~rcs/life.html">Life</a></li>
          <li><a href="http://colin-scott.github.com/music">Music</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </div>
</div>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://google.com/search" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:colin-scott.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2014/10/07/half-baked-ideas/">Half-Baked Ideas</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-07T09:23:48-07:00" pubdate data-updated="true">Oct 7<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/10/07/half-baked-ideas/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2014/10/01/performance-modeling-for-sdn/">Performance Modeling for Network Control Plane Systems</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-01T21:51:21-07:00" pubdate data-updated="true">Oct 1<span>st</span>, 2014</time>
        
         | <a href="/blog/2014/10/01/performance-modeling-for-sdn/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>At Berkeley I have the opportunity to work with some of the smartest undergrads around. One of the undergrads I work with,
<a href="https://plus.google.com/109177137524762864782/about">Andrew Or</a>, did some neat work on modeling the performance of network control plane systems (e.g. SDN controllers).
He decided to take a once-in-a-lifetime opportunity to join <a href="http://databricks.com/">Databricks</a> before we got the chance to publish his work, so in his stead I thought
I&rsquo;d share his work here.</p>

<p>An interactive version of his performance model can be found at this <a href="http://www.eecs.berkeley.edu/~rcs/research/convergence_modeling/">website</a>. Description from the website:</p>

<blockquote>
<p>A key latency metric for network control plane systems is convergence time: the duration between when a change occurs in a network and when the network has converged to an updated configuration that accommodates that change. The faster the convergence time, the better.<p><br />

<p>Convergence time depends on many variables: latencies between network devices, the number of network devices, the complexity of the replication mechanism used (if any) between controllers, storage latencies, etc. With so many variables it can be difficult to build an intuition for how the variables interact to determine overall convergence time.</p><br />

<p>The purpose of this tool is to help build that intuition. Based on analytic models of communication complexity for various replication and network update schemes, the tool quantifies convergence times for a given topology and workload. With it, you can answer questions such as &#8220;How far will my current approach scale while staying within my SLA?&#8221;, and &#8220;What is the convergence time of my network under a worst-case workload?&#8221;.</p>
</blockquote>


<p>The tool is insightful (e.g.
note the striking difference between SDN controllers and traditional routing protocols) and a lot of fun to play around with; I encourage you to check it out.
In case you are curious about the details of the model or would like to suggest
improvements, the code is available <a href="https://github.com/andrewor14/web-model">here</a>. We also have a 6-page write up of the work, available upon request.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2014/04/29/is-academia-a-good-place-to-build-real-software/">Is Academia a Good Place to Build Real Software?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-29T19:56:04-07:00" pubdate data-updated="true">Apr 29<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/04/29/is-academia-a-good-place-to-build-real-software/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I often overhear a recurring debate amongst researchers: is Academia a good
place to build real software systems? By &ldquo;real&rdquo;, we typically mean &ldquo;used&rdquo;,
particularly by people outside of academic circles.</p>

<p>There have certainly been some success stories. <a href="http://www.bsd.org/">BSD</a>,
<a href="http://llvm.org/">LLVM</a>, <a href="http://www.xenproject.org/">Xen</a>, and <a href="http://spark.apache.org/">Spark</a> come to mind.</p>

<p>Nonetheless, some argue that these success stories came about at a time
when the surrounding software ecosystem was nascent enough for a small group of researchers to
be able to make a substantial contribution, and that the ecosystem is normally
at a point where researchers cannot easily contribute. Consider for example that BSD was
initially released in 1977, when very few open source operating systems
existed. Now we have Linux, which has almost <a href="http://www.cnet.com/news/linux-development-by-the-numbers-big-and-getting-bigger/">1400 active
developers</a>.</p>

<p>Is this line of reasoning correct? Is the heyday of Academic systems software over? Will it ever come again?</p>

<p>Without a doubt, building real software systems requires substantial (wo)manpower; no
matter how great the idea is, implementing it will require raw effort.</p>

<p>This fact suggests an indirect way to evaluate our question. Let&rsquo;s assume that
(i) any given software
developer can only produce a fixed (constant) amount of coding progress in a fixed
timeframe and (ii) the maturity of the surrounding software ecosystem is
proportional to collective effort put into it. We can then approximate an
answer to our
question by looking at the number of software developers in industry vs. the number of
researchers over time.</p>

<p>It turns out that the <a href="http://www.bls.gov/">Bureau of Labor Statistics</a>
publishes exactly the <a href="http://www.bls.gov/data/">data</a> we need for the United States.
Here&rsquo;s what I found:</p>

<p><img src="http://www.eecs.berkeley.edu/~rcs/research/oes.jpg" alt="OES data" width=100%></p>

<p>Hm. The first thing we notice is that it&rsquo;s hard to even see the line for academic and industrial researchers.
To give you a sense of where it&rsquo;s at, the y-coordinate at May, 2013 for computer science teachers and professors is 35,770, two
orders of magnitude smaller than the 3,339,440 total employees in the software industry at that time.</p>

<p>What we really care about though is the ratio of employees in industry to number of researchers:</p>

<p><img src="http://www.eecs.berkeley.edu/~rcs/research/oes_ratio.jpg" alt="OES ratio data" width=100%></p>

<p>In the last few years, both the software industry and Academia are growing at roughly the same rate, whereas researchers in industrial
labs appear to be dropping off relative to the software industry. We can see this relative growth rate better by normalizing the datasets (dividing each datapoint by the maximum datapoint
in its series &mdash; might be better to take the derivative, but I&rsquo;m too lazy to
figure out how to do that at the moment):</p>

<p><img src="http://www.eecs.berkeley.edu/~rcs/research/oes_normalized.jpg" alt="OES normalized data" width=100%></p>

<p>The data for the previous graphs only goes back to 1995. The Bureau of Labor
Statistics also publishes coarser granularity going all the way to 1950 and beyond:</p>

<p><img src="http://www.eecs.berkeley.edu/~rcs/research/nes.jpg" alt="NES data" width=100%></p>

<p>(See the hump around 2001?)</p>

<p>Not sure if this data actually answers our initial question, but I certainly found it insightful!
If you&rsquo;d like more details on how I did this analysis, or would like to play around with the data for
yourself, see my <a href="https://github.com/colin-scott/go-bls-client">code</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2014/03/30/what-distinguishes-distributed-computing-from-parallel-computing/">What Distinguishes Distributed Computing From Parallel Computing?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-30T11:29:00-07:00" pubdate data-updated="true">Mar 30<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/03/30/what-distinguishes-distributed-computing-from-parallel-computing/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I recently came across a <a href="http://aphyr.com/posts/285-call-me-maybe-riak">statement</a> in <a href="https://twitter.com/aphyr">Aphyr&rsquo;s</a> excellent
<a href="http://aphyr.com/tags/jepsen">Jepsen</a> blog series that caught my eye:</p>

<blockquote><p>&ldquo;In a very real sense, [network] partitions are just really big windows of concurrency.&rdquo;</p></blockquote>

<p>This statement seems to imply that distributed systems are &ldquo;equivalent&rdquo;
to parallel (single-machine) computing systems, for the following reason: partitions,
which occur in a network but don&rsquo;t really occur on a single chip, appear to be the key
distinguishing property of distributed systems. But if partitions are just a
special case of concurrency, then there shouldn&rsquo;t be any fundamental reasons
why algorithms for multicore chips wouldn&rsquo;t be perfectly suitable for solving all the
problems we might encounter in a distributed setting.
We know this to be false,
so I&rsquo;ve been trying to puzzle out precisely what
properties of distributed computing distinguish it from parallel computing
[1].</p>

<p>I&rsquo;ve been taught that distributed systems have two crucial features:</p>

<ul>
<li>Asynchrony, or &ldquo;absence of synchrony&rdquo;: messages from one process to another
do not arrive immediately. In a fully asynchronous system, messages may be
delayed for unbounded periods of time.</li>
<li>Partial failure: some processes in the system may fail while other processes
continue executing.</li>
</ul>


<!--
Observe that in a loose sense, network partitions are a form of partial failure,
because from the perspective of the other nodes in the system, a partitioned node
is indistinguishable from a crashed node.
-->


<p>Let&rsquo;s discuss these two properties separately.</p>

<h3>Asynchrony</h3>

<p>Parallel systems also exhibit asynchrony, as long it&rsquo;s possible for
there to be a delay between one process sending a message [2]
and the other processes having the opportunity to read that message. Even on a single
machine, this delay might be induced by locks within the operating system kernel,
or by the cache coherence protocol implemented in hardware on a multicore chip.</p>

<p>With this in mind, let&rsquo;s return to Aphyr&rsquo;s statement.
What exactly did he mean by &ldquo;big windows of concurrency&rdquo;?
His article focuses on what happens when multiple clients write to the same
database key, so by &ldquo;concurrency&rdquo; I think he is referring to situations where multiple processes
might simultaneously issue writes to the same piece of state. But if you
think about it, the entire execution is a &ldquo;big window of
concurrency&rdquo; in this sense, regardless of whether the database replicas are partitioned.
By &ldquo;big windows of concurrency&rdquo; I think Aphyr was really talking about <em>asynchrony</em> (or more
precisely, periods of high message delivery delays),
since network partitions are hard to deal with precisely because the messages
between replicas aren&rsquo;t deliverable until after the partition is recovered:
when replicas can&rsquo;t coordinate, it&rsquo;s challenging (or impossible, if the system chooses to enforce linearizability)
for them to correctly process those concurrent writes. Amending Aphyr&rsquo;s statement then:</p>

<blockquote><p>&ldquo;Network partitions are just really big windows of asynchrony.&rdquo;</p></blockquote>

<p>Does this amendment resolve our quandary? Someone could
rightly point out that because partitions don&rsquo;t really occur within a single chip,
parallel systems can effectively provide guarantees on how long message
delays can last [3], whereas partitions in distributed systems may last
arbitrarily long. Some algorithms designed for parallel computers might
therefore break in a distributed setting,
but I don&rsquo;t think this is really the distinction we&rsquo;re looking
for.</p>

<h3>Partial Failure</h3>

<p>Designers of distributed algorithms codify their assumptions
about the possible ways nodes can fail by specifying a &lsquo;failure model&rsquo;. Failure models might describe
how many nodes can fail&mdash;for example, quorum-based algorithms assume that no more
than N/2 nodes ever fail, otherwise they cannot make progress&mdash;or they might
spell out how individual crashed nodes behave. The latter constraint forms a
hierarchy, where weaker failure models (e.g. &lsquo;fail-stop&rsquo;, where crashed nodes are guaranteed to never
send messages again) can be reduced to special cases of stronger models (e.g.
&lsquo;Byzantine&rsquo;, where faulty nodes can behave arbitrarily, even possibly
mimicking the behavior of correct nodes) [4].</p>

<p>Throughout the Jepsen series, Aphyr tests distributed systems by (i) telling
clients to issue concurrent writes, (ii) inducing a network partition between
database replicas, and (iii) recovering the partition. Observe that Jepsen
never actually kills replicas! This failure model is actually weaker than fail-stop,
since nodes are guaranteed to eventually resume sending messages [5].
Aphyr&rsquo;s statement is beginning to make sense:</p>

<blockquote><p>&ldquo;Network partitions that are followed by network recovery are just really big windows of asynchrony.&rdquo;</p></blockquote>

<p>This statement is true; from the perspective of a node in the system, a network partition followed by a network recovery
is indistinguishable from a random spike in message delays, or peer nodes that
are just very slow to respond. In other words, a distributed system that
guarantees that messages will eventually be deliverable to all nodes is
equivalent to an asynchronous parallel system. But if any nodes in the
distributed system actually fail, we&rsquo;re no longer equivalent to a parallel
system.</p>

<h3>Who cares?</h3>

<p>This discussion might sound like academic hairsplitting, but I claim that
these distinctions have practical implications.</p>

<p>As an example, let&rsquo;s imagine that you need to make a choice between shared memory
versus message passing as the communication model for the shiny new distributed
system you&rsquo;re designing. If you come from a parallel computing background you
would know that message passing is actually equivalent to shared memory, in
the sense that you can use a message passing abstraction to implement
shared memory, and vice versa. You might therefore conclude that you
are free to choose whichever abstraction is more convenient or performant for
your distributed system. If you jumped to this conclusion you might end up
making your system more fragile without realizing it.
Message passing is not equivalent to shared memory in distributed systems [6],
precisely because distributed systems exhibit <em>partial failures</em>;
in order to correctly implement shared memory in a distributed system it must
always be possible to coordinate with a quorum, or
otherwise be able to accurately detect which nodes have failed. Message
passing does not have this limitation.</p>

<p>Another takeaway from this discussion is that Jepsen is actually testing a
fairly weak failure mode. Despite Jepsen&rsquo;s simplicity though, Aphyr has managed to uncover problems in
an impressive number of distributed databases. If we want to uncover yet more implicit assumptions
about how our systems behave, stronger failure modes seem like an
excellent place to look.</p>

<hr />

<p>[1] Thanks to <a href="http://www.cs.berkeley.edu/~alig/">Ali Ghodsi</a> for helping me tease out the differences between these properties.</p>

<p>[2] or writing to shared memory, which is essentially the same as sending a
message.</p>

<p>[3] See, for example, PRAM or BSP, which assume that every node can
communicate with every other node within each &ldquo;round&rdquo;. It&rsquo;s trivial to solve
<a href="http://groups.csail.mit.edu/tds/papers/Lynch/pods83-flp.pdf">hard</a> problems like
consensus in this world, because you can always just take a majority
vote and decide within two rounds.</p>

<p>[4] See Ali Ghodsi&rsquo;s excellent <a href="http://www.cs.berkeley.edu/~alig/cs294-91/events-links.pptx">slides</a> for a taxonomy of these failure models.</p>

<p>[5] Note that this is not equivalent to &lsquo;crash-recovery&rsquo;. Crash-recovery is
actually stronger than fail-stop, because nodes <em>may</em> recover or they may
not.</p>

<p>[6] Nancy Lynch, &ldquo;Distributed Algorithms&rdquo;, Morgan Kaufmann, 1996.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/12/wan-vs-datacenter-link-reliability/">WAN vs. Datacenter Link Reliability</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-12T15:46:00-07:00" pubdate data-updated="true">May 12<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/12/wan-vs-datacenter-link-reliability/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>According to a study by Turner et al. [1], wide area network links have an
average of 1.2 to 2.7 days of downtime per year. This translates to roughly
two and a half 9&rsquo;s of reliability [2].</p>

<p>I was curious how this compared to datacenter links, so I took a look at Gill
et. al&rsquo;s paper [3] on datacenter network failures at Microsoft. Unfortunately some
of the data has been redacted, but I was able to reverse engineer the mean
link downtime per year with the help of <a href="http://www.eecs.berkeley.edu/~apanda/">Aurojit Panda&rsquo;s</a>
<a href="https://github.com/apanda/svg-points">svg-to-points</a> converter. The results
are interesting: out of all links types, the average downtime was 0.3 days.
This translates to roughly three and a half 9&rsquo;s of reliability, an order of magnitude greater
than WAN links.</p>

<p>Intuitively this makes sense. WAN links are much more prone to
<a href="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf">drunken hunters, bulldozers, wild dogs,</a>
<a href="http://www.zetatalk.com/newsletr/issue284.htm">ships dropping anchor</a> and the like than links within a <a href="http://www.wired.com/wiredenterprise/2012/10/data-center-easter-eggs/">secure</a> datacenter.</p>

<h4>Footnotes</h4>

<p>[1] Daniel Turner, Kirill Levchenko, Alex C. Snoeren, and Stefan Savage. California Fault Lines: Understanding the Causes and Impact of Network Failures, Table 4. SIGCOMM &lsquo;10.</p>

<p>[2] Note that this statistic is specifically about hardware failure, not overall network availability.</p>

<p>[3] Phillipa Gill, Navendu Jain, Nachiappan Nagappan. Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications, Figures 8c &amp; 9c. SIGCOMM &lsquo;11</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2012/12/24/latency-trends/">Latency Trends</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-24T15:20:00-08:00" pubdate data-updated="true">Dec 24<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/12/24/latency-trends/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In 2010, Jeff Dean gave a <a href="http://goo.gl/0MznW">talk</a> that laid out
a list of <a href="https://gist.github.com/2843375">numbers</a> every programmer
should know. His list has since become relatively well known among the systems community.</p>

<p>The other day, a friend mentioned a latency number to me, and I realized that
it was an order of magnitude smaller than what I had memorized from
Jeff&rsquo;s talk. The problem, of course, is that hardware performance increases
exponentially! After some digging, I actually found that the numbers Jeff
quotes are over a decade old [1].</p>

<p>Partly inspired by my officemate <a href="http://www.eecs.berkeley.edu/~apanda/">Aurojit Panda</a>, who is collecting
awesome <a href="http://www.eecs.berkeley.edu/~rcs/research/hw_trends.xlsx">data</a> on
hardware performance, I decided to write a little tool [2] to visualize Jeff&rsquo;s
numbers as a function of time [3].</p>

<p>Without further ado,
<a href="http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html">here</a> it
is.</p>

<h4>Footnotes</h4>

<p>[1] Jeff&rsquo;s numbers are from 2001, and were first publicized by Peter Norvig in this
<a href="http://norvig.com/21-days.html#answers">article</a>.</p>

<p>[2] Layout stolen directly from <a href="https://github.com/ayshen">ayshen</a> on GitHub.</p>

<p>[3] The hardware trends I&rsquo;ve gathered are rough estimates. If you want to tweak
the parameters yourself, I&rsquo;ve made it really easy to do so &mdash; please send me
updates! Better yet, issue a <a href="https://github.com/colin-scott/interactive_latencies">pull request</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2012/10/28/7-ways-to-deal-with-ordering-bugs-in-distributed-systems/">7 Ways to Handle Concurrency in Distributed Systems</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-28T17:54:00-07:00" pubdate data-updated="true">Oct 28<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/28/7-ways-to-deal-with-ordering-bugs-in-distributed-systems/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em>tl;dr: handling event ordering correctly in distributed systems is tricky.
In this post I cover 7 approaches to coping with concurrency.</em></p>

<p>Robust distributed systems are notoriously difficult to build. The difficulty
arises from two properties in particular:</p>

<ul>
<li><p>Limited knowledge: each node knows its own state, and it knows what state
the other nodes were in recently, but it can&rsquo;t know their current state.</p></li>
<li><p>(Partial) failures: individual nodes can fail at any time, and the network can delay or drop
messages arbitrarily.</p></li>
</ul>


<p>Why are these properties difficult to grapple with? Suppose you&rsquo;re writing
code for a single node. You&rsquo;re deep in a nested conditional statement, and you
need to deal with a message arrival. How do you react? What if the message
you&rsquo;re seeing was actually delayed by the network and is no longer relevant? What if some of the nodes you need to coordinate with have
failed, but you aren&rsquo;t aware of it yet? The set of possible event sequences you need to reason about is huge,
and it&rsquo;s all too easy to forget about the one nasty corner case that will
eventually bring your system to a screeching halt.</p>

<p>To make the discussion more concrete, let&rsquo;s look at an example.</p>

<p><img src="http://www.eecs.berkeley.edu/~rcs/research/example_bug.png"
alt="Floodlight bug" width=100%></p>

<p>The figure above depicts a race condition [1] in
<a href="http://floodlight.openflowhub.org/">Floodlight</a>, a distributed controller for
software-defined networks. With Floodlight, switches maintain one hot
connection to a master controller and
one or more cold connections to replica controllers. The master holds the
authority to modify the configuration of the switches, while the other
controllers are in slave mode and do not perform any changes to the
switch configurations unless they detect that the master has crashed [2].</p>

<p>The race condition is triggered when a link fails (E1), and the switch
attempts to notify the controllers (E2,E4) shortly after the master has
died (E3), but before a new master has been selected (E6). In this case,
all live controllers are in
the slave role and will not take responsibility for updating the switch
flow table (E5). At some point, heartbeat messages time out and one of the
slaves
elevates itself to the master role (E6). The new master will proceed to
manage
the switch, but without ever clearing the routing entries for
the failed link (resulting in a persistent blackhole) [3].</p>

<p>If we take a step back, we see that there are two problems involved:
leader election (&ldquo;Who is the master at any point in time?&rdquo;), and
replication (&ldquo;How should the backups behave?&rdquo;). Let&rsquo;s assume that leader
election is handled by a separate consensus algorithm (<em>e.g.</em>
Paxos), and focus our attention on replication.</p>

<!-- And atomic commit? -->


<p>Now that we have a concrete example to think about, let&rsquo;s go over a few
solutions this problem. The first four share the same philosophy: <em>&ldquo;get
the ordering right&rdquo;</em>.</p>

<h3>Take it case-by-case</h3>

<p>The straightforward fix here is to add a conditional statement for this event
ordering: if you&rsquo;re a slave, and the next message is a link failure
notification, store the notification in memory in case you become master
later.</p>

<p>This fix seems easy in retrospect. But recall how the bug came about in the
first place: the programmer had some set of event orderings in mind when
writing the code, but didn&rsquo;t implement one corner case. How do we know
there isn&rsquo;t another race condition lurking somewhere else in the code? [4]</p>

<p>The number of event orderings you need to consider in a distributed system is
truly huge; it scales combinatorially with the number of nodes you&rsquo;re
communicating with. For the rest of this post, let&rsquo;s see if we can avoid the
need to reason on a case-by-case basis altogether.</p>

<h3>Replicate the computation</h3>

<p>Consider a system consisting of only one node. In this world,
there is a single, global order (with no race conditions)!</p>

<p>How can we obtain a global event order, yet still achieve fault tolerance? One way [5] is to have the backup nodes mimic every step of the master node: forward all
inputs to the master, have the master choose a serial order for those events, issue the appriopriate commands to the switches, and replicate the decision to the backups [6]. The key here is that each
backup should execute the computation in the exact same order as the master.</p>

<p>For the Floodlight bug, the backup would still need to hold the link failure
message in memory until it detects that the master has crashed. But we&rsquo;ve
gained a powerful guarantee over the previous approach: when the backup takes
over for the master, it will be in a up-to-date state, and know exactly what
commands it needs to send to the switches to get them into a correct
configuration.</p>

<h3>Make your event handlers transactional</h3>

<p>Transactions allow us to make a group of operations appear either as if they
happened simultaneously, or not at all. This is a powerful idea!</p>

<p>How could transactions help us here? Suppose we did the following: whenever a
message arrives, find the event handler
for that message, wrap it in a transaction, run the event handler, and hand
the result of the transaction to the master controller. The master
decides on a global order, checks whether any concurrent transactions
conflict with each other (and aborts one of them if they do), updates the switches, sends the
serialized transactions to the backups, and waits for ACKs before logging
a commit message.</p>

<p>This is very similar to the previous solution, but it gives us two benefits over the previous approach:</p>

<ul>
<li>We can potentially handle more events in parallel; most of the transactions will
not conflict with each other, and we can simply abort and retry the ones
that do.</li>
<li>We can now roll back operations. Suppose a network operator issues a
policy change to the controller, but realizes that she made a mistake.
No problem &mdash; she can simply roll back the previous transaction and start
again where she began.</li>
</ul>


<p>Compared to the first approach, this is a significant improvement! Each
event is handled in isolation from the other events, so there&rsquo;s need to
reason about event interleavings; if a conflicting transaction was
committed before we get to commit, just abort and retry!</p>

<h3>Reorder events when no one will notice</h3>

<p>It turns out that we can achieve even better throughput if we use a
replication model called virtual synchrony. In short, virtual synchrony
provides a library with three operations:</p>

<ul>
<li><tt>join()</tt> a process group</li>
<li><tt>register()</tt> an event handler</li>
<li><tt>send()</tt> an atomic multicast message to the rest of your process
group.</li>
</ul>


<p>These primitives provide two crucial guarentees:</p>

<ul>
<li>Atomic multicast means that if <em>any</em> correct node gets the message, every live
node will eventually get the message. That implies that if any live
node ever gets the link failure notification, you can rest assure that
one of your future masters will get it.</li>
<li>The <tt>join()</tt> protocol ensures that every node always know who&rsquo;s a member of its group,
and that everyone has the same view of who is alive
and who is not. Failures results in a group change, but everyone will agree on the order in which the failure occured.</li>
</ul>


<p>With virtual synchrony, we no longer need a single master; atomic multicast
means that there is a single order of events observed by all members of the
group, regardless of who initiated the message. And with multiple masters, we
aren&rsquo;t constrained by the speed of a single node.</p>

<p>The <em>virtual</em> part of virtual synchrony is that when the library detects
that two operations are not causally related to each other, it can reorder
them in whatever way it believes most efficient. Since those operations aren&rsquo;t
causally related, we&rsquo;re guaranteed that the final output won&rsquo;t be noticeably
different.</p>

<p>OK, let&rsquo;s move on to the final three approaches, which take a different tack
than the first four: <em>&ldquo;avoid having to reason about event ordering
altogether&rdquo;</em></p>

<h3>Make yourself stateless</h3>

<p>In a database, the &ldquo;ground truth&rdquo; is stored on disk. In a network, the &ldquo;ground
truth&rdquo; is stored in the routing tables of the switches themselves. This
implies that the controllers&#8217; view of the network is just soft state; we can
always recover it simply by querying the switches for their current
configuration!</p>

<p>How does this observation relate to the Floodlight bug? Suppose we didn&rsquo;t even
attempt to keep the backup controllers in sync with the master. Instead, just
have them recompute the entire network configuration whenever they realize
they need to take over for the master. Their only job in the meantime is to
monitor the liveness of the master!</p>

<p>Of course, the tradeoff here is that it may take significantly longer for the newly elected master to get up to speed.</p>

<p>We can apply the same trick to avoid race conditions between concurrent events
at the master: instead of maintaining locks between threads, just restart
computation of the entire network configuration whenever a new event comes in.
Race conditions don&rsquo;t happen if there is no shared state!</p>

<p>Incidentally, Google&rsquo;s <a href="http://www.eecs.berkeley.edu/~rcs/research/google-onrc-slides.pdf">wide-area network
controller</a>
is almost entirely stateless, presumably for many of the same reasons.</p>

<h3>Force yourself to be stateless</h3>

<p>In the spirit stateless computation, why not write your code in a language
that doesn&rsquo;t allow you to keep state at all? Programs written in declarative
languages such as <a href="http://p2.berkeley.intel-research.net/">Overlog</a> have no
explicit ordering whatsoever. Programmers simply declare rules such as &ldquo;If the
switch has a link failure, then flush the routing entries that go over that
link&rdquo;, and the language runtime handles the order in which the computation
is carried out.</p>

<p>With a declarative language, as the long as the same set of events is fed
to the controller (regardless of their order), the same result will come
out. This makes replication really easy: send inputs to all controllers,
have each node compute the resulting configuration, and only allow the
master node to send out commands to the switches once the computation has
completed. The tradeoff is that without an explicit ordering.
 the performance of declarative languages is difficult to reason about.</p>

<h3>Guarantee self-stabilization</h3>

<p>The previous solutions were designed to always guarantee correct behavior
despite failures of the other nodes. This final solution, my personal
favorite, is much more optimistic.</p>

<p>The idea behind self-stabilizing algorithms is to have a provable guarantee
that no matter what configuration the system starts in, and no matter what
failures occur, all nodes will eventually stabilize to a configuration where
safety properties are met. This eliminates the need to worry about correct
initialization, or detect whether the algorithm has terminated. As a nice side
benefit, self-stabilizing algorithms are usually considerably simpler than
their order-aware counterparts.</p>

<p>What do self-stabilizing algorithms look like? Self-stabilizing algorithms are
actually everywhere in networking &mdash; routing algorithms are the most canonical
example.</p>

<p>How would a self-stabilizing algorithm help with the Floodlight bug? The
answer really depends on what network invariants the control application needs
to maintain. If it&rsquo;s just to provide connectivity [7], we could simply run a
traditional link-state algorithm: have each switch periodically send port
status messages to the controllers, have the controllers compute shortest
paths using Dijkstra&rsquo;s, and have the master push the appropriate updates to
the switches. Even if there are transient failures, we&rsquo;re guaranteed that the
network will eventually converge to a configuration with no loops and
deadends.</p>

<hr />

<p>Ultimately, the best replication choice depends on your workload and
network policies. In any case, I hope this post has convinced you that
there&rsquo;s more than one way to skin a cat!</p>

<h4>Footnotes</h4>

<p>[1] Note that this issue was originally discovered by the developers of
Floodlight. (We don&rsquo;t mean to pick on BigSwitch here; we chose this bug
because it&rsquo;s a great example of the difficulties that come up in distributed
systems). For more information, see line 605 of
<a href="https://github.com/floodlight/floodlight/blob/2e9427e20ede7dc3941f8c15d2348bfcafdce237/src/main/java/net/floodlightcontroller/core/internal/Controller.java">Controller.java</a>.</p>

<p>[2] This invariant is crucial to maintain. Think of the switches&#8217; routing
tables as shared variables between threads (controllers). We need to ensure
mutual exclusion over those shared variables, otherwise we could end up with
internally inconsistent routing tables.</p>

<p>[3] The Floodlight bug noted in [1] actually involves neglecting to clear the
routing tables of newly connected switches, but the same flavor of race
condition could occur for link failures. We chose to focus on link failures
because they&rsquo;re likely to occur much more often than switch connects.</p>

<p>[4] It&rsquo;s possible in some cases to use a <a href="http://www.macesystems.org/">model checker</a> to automatically find race conditions, but the runtime complexity is often intractable and very few systems do this in practice.</p>

<p>[5] There are actually a handful of ways to implement state machine replication. Ours depend on a concensus algorithm to choose the master, but you could also run the concensus algorithm itself to achieve replication. There are also cheaper algorithms such as reliable broadcast. You can also get significantly better read throughput with chain replication, which doesn&rsquo;t require quorum for reads, but writes become more complicated.</p>

<p>[6] We still need to maintain the invariant that only the master modifies the
the switch configurations. Nonetheless, with state machine replication the
backup will always know what commands need to be sent to switches if and when
it takes over for the master.</p>

<p>[7] Although if your goal is only to provide connectivity, it&rsquo;s <a href="http://networkheresy.com/2011/11/17/is-openflowsdn-good-at-forwarding/">not clear</a>
why you&rsquo;re using SDN in the first place.</p>
</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section class="well">
  <ul id="recent_posts" class="nav nav-list">
    <li class="nav-header">Recent Posts</li>
    
      <li class="post">
        <a href="/blog/2014/10/07/half-baked-ideas/">Half-Baked Ideas</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/01/performance-modeling-for-sdn/">Performance Modeling for Network Control Plane Systems</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/29/is-academia-a-good-place-to-build-real-software/">Is Academia A Good Place To Build Real Software?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/30/what-distinguishes-distributed-computing-from-parallel-computing/">What Distinguishes Distributed Computing From Parallel Computing?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/12/wan-vs-datacenter-link-reliability/">WAN vs. Datacenter Link Reliability</a>
      </li>
    
  </ul>
</section>

<section class="well">
  <ul id="gh_repos" class="nav">
    <li class="nav-header">GitHub Repos</li>
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/colin-scott">@colin-scott</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        github.showRepos({
            user: 'colin-scott',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/asides/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><hr>
<p>
  Copyright &copy; 2014 - Colin Scott -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'restforthewicked';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
